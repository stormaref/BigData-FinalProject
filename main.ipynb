{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import implicit\n",
    "from sklearn.model_selection import KFold\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH and Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1112486027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>1112484676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>1112484819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>1112484727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>1112484580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000258</th>\n",
       "      <td>138493</td>\n",
       "      <td>68954</td>\n",
       "      <td>4</td>\n",
       "      <td>1258126920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000259</th>\n",
       "      <td>138493</td>\n",
       "      <td>69526</td>\n",
       "      <td>4</td>\n",
       "      <td>1259865108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000260</th>\n",
       "      <td>138493</td>\n",
       "      <td>69644</td>\n",
       "      <td>3</td>\n",
       "      <td>1260209457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000261</th>\n",
       "      <td>138493</td>\n",
       "      <td>70286</td>\n",
       "      <td>5</td>\n",
       "      <td>1258126944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000262</th>\n",
       "      <td>138493</td>\n",
       "      <td>71619</td>\n",
       "      <td>2</td>\n",
       "      <td>1255811136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          userId  movieId  rating   timestamp\n",
       "0              1        2       3  1112486027\n",
       "1              1       29       3  1112484676\n",
       "2              1       32       3  1112484819\n",
       "3              1       47       3  1112484727\n",
       "4              1       50       3  1112484580\n",
       "...          ...      ...     ...         ...\n",
       "20000258  138493    68954       4  1258126920\n",
       "20000259  138493    69526       4  1259865108\n",
       "20000260  138493    69644       3  1260209457\n",
       "20000261  138493    70286       5  1258126944\n",
       "20000262  138493    71619       2  1255811136\n",
       "\n",
       "[20000263 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df = pd.read_csv('Dataset/ratings.csv').astype(int)\n",
    "rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_matrix(data):\n",
    "    pivot_table = data.pivot(index='userId', columns='movieId', values='normalized_rating').fillna(0)\n",
    "    return csr_matrix(pivot_table.values)\n",
    "\n",
    "def normalize_data(data):\n",
    "    mean_ratings = data.groupby('userId')['rating'].mean().reset_index(name='mean_rating')\n",
    "    data = data.merge(mean_ratings, on='userId')\n",
    "    data['normalized_rating'] = data['rating'] - data['mean_rating']\n",
    "    data.drop(columns=['rating', 'mean_rating'], inplace=True, errors='ignore')\n",
    "    return data, mean_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_vectors(length: int, count: int) -> list:\n",
    "    return [np.random.uniform(-1, 1, size=length) for _ in range(count)]\n",
    "        \n",
    "def create_hash(vector: list, min_hash):\n",
    "    result = []\n",
    "    for hash in min_hash:\n",
    "        a = np.dot(vector, hash) > 0\n",
    "        result.append(1 if a else 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bucket_number(hash_vector):\n",
    "    s = ''.join(str(bit) for bit in hash_vector)\n",
    "    bucket_number = int(s, 2)\n",
    "    return bucket_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighbourhood Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neighborhood_matrix(new_user_vector, buckets, min_hash, sparse_matrix) -> csr_matrix:\n",
    "    hash_vector = create_hash(new_user_vector, min_hash)\n",
    "    bucket_number = calculate_bucket_number(hash_vector)\n",
    "\n",
    "    if bucket_number in buckets:\n",
    "        neighbor_users = buckets[bucket_number]\n",
    "    else:\n",
    "        return csr_matrix((0, sparse_matrix.shape[1]))\n",
    "    \n",
    "    rows = [sparse_matrix[n].toarray()[0] for n in neighbor_users]\n",
    "    return csr_matrix(np.vstack(rows)) if rows else csr_matrix((0, sparse_matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def matrix_factorization(sparse_ratings: csr_matrix):\n",
    "#     model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=64, use_native=True, num_threads=0)\n",
    "#     model.fit(sparse_ratings.T, show_progress=False)\n",
    "#     user_factors = model.user_factors\n",
    "#     item_factors = model.item_factors\n",
    "#     return item_factors @ user_factors.T\n",
    "\n",
    "# def matrix_factorization(sparse_ratings: csr_matrix, k):\n",
    "#     U, S, V = scipy.sparse.linalg.svds(sparse_ratings, k=k)\n",
    "#     item_factors = U.dot(np.diag(S))\n",
    "#     user_factors = V.T\n",
    "#     factorization = item_factors.dot(user_factors.T)\n",
    "#     factorization = item_factors @ user_factors.T\n",
    "#     return factorization\n",
    "\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "def matrix_factorization(sparse_ratings: csr_matrix, k: int):\n",
    "    U, S, V = randomized_svd(sparse_ratings, n_components=k)\n",
    "    item_factors = U.dot(np.diag(S))\n",
    "    user_factors = V\n",
    "    factorization = item_factors.dot(user_factors)\n",
    "    return factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation_coefficient(array1, array2):\n",
    "    if len(array1) != len(array2):\n",
    "        raise ValueError(\"Arrays must be of the same length\")\n",
    "    \n",
    "    mean1 = np.mean(array1)\n",
    "    mean2 = np.mean(array2)\n",
    "    \n",
    "    centered1 = array1 - mean1\n",
    "    centered2 = array2 - mean2\n",
    "    \n",
    "    covariance = np.sum(centered1 * centered2) / len(array1)\n",
    "    std_dev1 = np.sqrt(np.sum(centered1**2) / len(array1))\n",
    "    std_dev2 = np.sqrt(np.sum(centered2**2) / len(array2))\n",
    "    \n",
    "    if std_dev1 * std_dev2 == 0:\n",
    "        return 0\n",
    "    \n",
    "    pearson_coefficient = covariance / (std_dev1 * std_dev2)\n",
    "    \n",
    "    return pearson_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(user_vector, similar_users_count, buckets, min_hash, sparse_matrix):\n",
    "    nm = create_neighborhood_matrix(user_vector, buckets, min_hash, sparse_matrix)\n",
    "    min_shape = np.min(nm.shape)\n",
    "    if min_shape == 0:\n",
    "        return []\n",
    "    k = int(0.5 * min_shape)\n",
    "    if k < 1:\n",
    "        k = 1\n",
    "    if k == min_shape:\n",
    "       return [] \n",
    "    neighborhood_matrix = matrix_factorization(nm, k)\n",
    "    \n",
    "    similarities = np.array([np.abs(pearson_correlation_coefficient(neighborhood_matrix[i], user_vector))\n",
    "                             for i in range(neighborhood_matrix.shape[0])])\n",
    "    \n",
    "    similar_users = similarities.argsort()[::-1]\n",
    "    user_mean = user_vector[user_vector != 0].mean()\n",
    "    \n",
    "    predicts = []\n",
    "    for item_idx in range(neighborhood_matrix.shape[1]):\n",
    "        if user_vector[item_idx] == 0:\n",
    "            predicts.append(0)\n",
    "            continue\n",
    "        weighted_ratings_sum = 0\n",
    "        weights_sum = 0\n",
    "        users_added = 0\n",
    "        for user_index in similar_users:\n",
    "            if users_added == similar_users_count:\n",
    "                break\n",
    "            \n",
    "            user_rating = neighborhood_matrix[user_index, item_idx]\n",
    "            if user_rating == 0:\n",
    "                continue\n",
    "            users_added += 1\n",
    "            similarity_score = similarities[user_index]\n",
    "            weighted_ratings_sum += user_rating * similarity_score\n",
    "            weights_sum += np.abs(similarity_score)\n",
    "        \n",
    "        predicted_rating = weighted_ratings_sum / weights_sum if weights_sum > 0 else 0\n",
    "        predicts.append(predicted_rating + user_mean)\n",
    "    \n",
    "    return np.array(predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_sparse(rating_df, df, key= 'normalized_rating'):\n",
    "    unique_user_ids = pd.Series(rating_df['userId'].unique()).sort_values()\n",
    "    user_mapping = pd.Series(index=unique_user_ids, data=range(len(unique_user_ids)))\n",
    "\n",
    "    unique_movie_ids = pd.Series(rating_df['movieId'].unique()).sort_values()\n",
    "    movie_mapping = pd.Series(index=unique_movie_ids, data=range(len(unique_movie_ids)))\n",
    "    \n",
    "    user_indices = df['userId'].map(user_mapping)\n",
    "    movie_indices = df['movieId'].map(movie_mapping)\n",
    "    values = df[key]\n",
    "\n",
    "    result = coo_matrix((values, (user_indices, movie_indices)), \n",
    "                    shape=(len(user_mapping), len(movie_mapping)))\n",
    "    print(result.shape)\n",
    "    print(result.count_nonzero())\n",
    "    return result.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138493, 26744)\n",
      "17935929\n",
      "(138493, 26744)\n",
      "1985928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bucketing: 100%|██████████| 138493/138493 [00:44<00:00, 3127.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% progress\n",
      "Fold 1 Loss: 12056.729804501523\n",
      "Validation Loss: 0.8902554681017147\n",
      "(138493, 26744)\n",
      "17903857\n",
      "(138493, 26744)\n",
      "2019027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bucketing: 100%|██████████| 138493/138493 [00:27<00:00, 5125.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% progress\n",
      "Fold 2 Loss: 12123.43903131032\n",
      "Validation Loss: 0.8855689577290226\n",
      "(138493, 26744)\n",
      "17952425\n",
      "(138493, 26744)\n",
      "1970491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bucketing: 100%|██████████| 138493/138493 [00:25<00:00, 5402.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% progress\n",
      "Fold 3 Loss: 12047.394200410943\n",
      "Validation Loss: 0.8862287921443978\n",
      "(138493, 26744)\n",
      "17967287\n",
      "(138493, 26744)\n",
      "1953751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bucketing: 100%|██████████| 138493/138493 [00:25<00:00, 5371.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% progress\n",
      "Fold 4 Loss: 12064.82412206025\n",
      "Validation Loss: 0.8824476391208492\n",
      "(138493, 26744)\n",
      "17980433\n",
      "(138493, 26744)\n",
      "1947154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bucketing: 100%|██████████| 138493/138493 [00:25<00:00, 5534.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% progress\n",
      "Fold 5 Loss: 12033.564882568275\n",
      "Validation Loss: 0.8885449961284999\n",
      "(138493, 26744)\n",
      "17952576\n",
      "(138493, 26744)\n",
      "1970407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bucketing: 100%|██████████| 138493/138493 [00:25<00:00, 5353.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% progress\n",
      "Fold 6 Loss: 12115.45611693077\n",
      "Validation Loss: 0.8853091791692197\n",
      "(138493, 26744)\n",
      "17950604\n",
      "(138493, 26744)\n",
      "1972734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bucketing: 100%|██████████| 138493/138493 [00:26<00:00, 5230.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% progress\n",
      "Fold 7 Loss: 12049.74131657642\n",
      "Validation Loss: 0.8804428844495411\n",
      "(138493, 26744)\n",
      "17997174\n",
      "(138493, 26744)\n",
      "1926122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bucketing: 100%|██████████| 138493/138493 [00:26<00:00, 5293.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% progress\n",
      "Fold 8 Loss: 12134.076460141361\n",
      "Validation Loss: 0.8914249529930474\n",
      "(138493, 26744)\n",
      "17903495\n",
      "(138493, 26744)\n",
      "2018993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bucketing: 100%|██████████| 138493/138493 [00:26<00:00, 5161.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% progress\n",
      "Fold 9 Loss: 12056.755484680109\n",
      "Validation Loss: 0.882761420755609\n",
      "(138493, 26744)\n",
      "17924968\n",
      "(138493, 26744)\n",
      "1996531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bucketing: 100%|██████████| 138493/138493 [00:26<00:00, 5160.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% progress\n",
      "Fold 10 Loss: 12011.526628983558\n",
      "Validation Loss: 0.8809333794634072\n",
      "Average Cross-Validation Loss: 0.8853917670055308\n"
     ]
    }
   ],
   "source": [
    "unique_users = rating_df['userId'].unique()\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "total_loss = 0\n",
    "for train_index, val_index in kf.split(unique_users):\n",
    "    time.sleep(10)\n",
    "    train_users = unique_users[train_index]\n",
    "    val_users = unique_users[val_index]\n",
    "    \n",
    "    train_data = rating_df[rating_df['userId'].isin(train_users)]\n",
    "    val_data = rating_df[rating_df['userId'].isin(val_users)]\n",
    "\n",
    "    train_data, mean_ratings_train = normalize_data(train_data)\n",
    "    train_sparse_matrix = df_to_sparse(rating_df, train_data)\n",
    "\n",
    "    val_sparse_matrix = df_to_sparse(rating_df, val_data, 'rating')\n",
    "\n",
    "    min_hash = create_hash_vectors(train_sparse_matrix.shape[1], 14)\n",
    "    buckets = {}\n",
    "    for i in tqdm(range(train_sparse_matrix.shape[0]), desc='Bucketing'):\n",
    "        user = train_sparse_matrix[i].toarray()[0]\n",
    "        hash_vector = create_hash(user, min_hash)\n",
    "        bucket_number = calculate_bucket_number(hash_vector)\n",
    "        if bucket_number not in buckets:\n",
    "            buckets[bucket_number] = []\n",
    "        buckets[bucket_number].append(i)\n",
    "\n",
    "    desire_indices = []\n",
    "    for i in range(val_sparse_matrix.shape[0]):\n",
    "        ith = val_sparse_matrix[i]\n",
    "        if ith.count_nonzero() > 0:\n",
    "            desire_indices.append(i)\n",
    "            \n",
    "    time.sleep(10)\n",
    "\n",
    "    loss = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    for idx, i in enumerate(desire_indices):\n",
    "      if idx == int(0.5 * len(desire_indices)):\n",
    "        print('50% progress')\n",
    "      user_vector = val_sparse_matrix[i].toarray()[0]\n",
    "      if user_vector.sum() == 0:\n",
    "          continue\n",
    "      predicted = predict_ratings(user_vector, 20, buckets, min_hash, train_sparse_matrix)\n",
    "      if len(predicted) == 0:\n",
    "          continue\n",
    "\n",
    "      mask = user_vector != 0\n",
    "      actual = user_vector[mask]\n",
    "      pred = predicted[mask]\n",
    "      user_loss = np.mean((actual - pred) ** 2)\n",
    "      loss += user_loss\n",
    "      processed_count += 1\n",
    "        \n",
    "    print(f'Fold {fold} Loss: {loss}')\n",
    "    fold += 1\n",
    "\n",
    "    loss /= processed_count\n",
    "    total_loss += loss\n",
    "\n",
    "    print(f'Validation Loss: {loss}')\n",
    "\n",
    "average_loss = total_loss / n_splits\n",
    "print(f'Average Cross-Validation Loss: {average_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
