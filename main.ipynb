{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import implicit\n",
    "from sklearn.model_selection import KFold\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1112486027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>1112484676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>1112484819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>1112484727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>1112484580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000258</th>\n",
       "      <td>138493</td>\n",
       "      <td>68954</td>\n",
       "      <td>4</td>\n",
       "      <td>1258126920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000259</th>\n",
       "      <td>138493</td>\n",
       "      <td>69526</td>\n",
       "      <td>4</td>\n",
       "      <td>1259865108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000260</th>\n",
       "      <td>138493</td>\n",
       "      <td>69644</td>\n",
       "      <td>3</td>\n",
       "      <td>1260209457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000261</th>\n",
       "      <td>138493</td>\n",
       "      <td>70286</td>\n",
       "      <td>5</td>\n",
       "      <td>1258126944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000262</th>\n",
       "      <td>138493</td>\n",
       "      <td>71619</td>\n",
       "      <td>2</td>\n",
       "      <td>1255811136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000263 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          userId  movieId  rating   timestamp\n",
       "0              1        2       3  1112486027\n",
       "1              1       29       3  1112484676\n",
       "2              1       32       3  1112484819\n",
       "3              1       47       3  1112484727\n",
       "4              1       50       3  1112484580\n",
       "...          ...      ...     ...         ...\n",
       "20000258  138493    68954       4  1258126920\n",
       "20000259  138493    69526       4  1259865108\n",
       "20000260  138493    69644       3  1260209457\n",
       "20000261  138493    70286       5  1258126944\n",
       "20000262  138493    71619       2  1255811136\n",
       "\n",
       "[20000263 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df = pd.read_csv('Dataset/ratings.csv').astype(int)\n",
    "rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_matrix(data):\n",
    "    pivot_table = data.pivot(index='userId', columns='movieId', values='normalized_rating').fillna(0)\n",
    "    return csr_matrix(pivot_table.values)\n",
    "\n",
    "def normalize_data(data):\n",
    "    mean_ratings = data.groupby('userId')['rating'].mean().reset_index(name='mean_rating')\n",
    "    data = data.merge(mean_ratings, on='userId')\n",
    "    data['normalized_rating'] = data['rating'] - data['mean_rating']\n",
    "    data.drop(columns=['rating', 'mean_rating'], inplace=True, errors='ignore')\n",
    "    return data, mean_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_vectors(length: int, count: int) -> list:\n",
    "    return [np.random.uniform(-1, 1, size=length) for _ in range(count)]\n",
    "        \n",
    "def create_hash(vector: list, min_hash):\n",
    "    result = []\n",
    "    for hash in min_hash:\n",
    "        a = np.dot(vector, hash) > 0\n",
    "        result.append(1 if a else 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bucket_number(hash_vector):\n",
    "    s = ''.join(str(bit) for bit in hash_vector)\n",
    "    bucket_number = int(s, 2)\n",
    "    return bucket_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neighborhood_matrix(new_user_vector, buckets, min_hash, sparse_matrix) -> csr_matrix:\n",
    "    hash_vector = create_hash(new_user_vector, min_hash)\n",
    "    bucket_number = calculate_bucket_number(hash_vector)\n",
    "\n",
    "    if bucket_number in buckets:\n",
    "        neighbor_users = buckets[bucket_number]\n",
    "    else:\n",
    "        return csr_matrix((0, sparse_matrix.shape[1]))\n",
    "    \n",
    "    rows = [sparse_matrix[n].toarray()[0] for n in neighbor_users]\n",
    "    return csr_matrix(np.vstack(rows)) if rows else csr_matrix((0, sparse_matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def matrix_factorization(sparse_ratings: csr_matrix):\n",
    "#     model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=64, use_native=True, num_threads=0)\n",
    "#     model.fit(sparse_ratings.T, show_progress=False)\n",
    "#     user_factors = model.user_factors\n",
    "#     item_factors = model.item_factors\n",
    "#     return item_factors @ user_factors.T\n",
    "\n",
    "def matrix_factorization(sparse_ratings: csr_matrix, k):\n",
    "    U, S, V = scipy.sparse.linalg.svds(sparse_ratings, k=k)\n",
    "    item_factors = U.dot(np.diag(S))\n",
    "    user_factors = V.T\n",
    "    factorization = item_factors.dot(user_factors.T)\n",
    "    factorization = item_factors @ user_factors.T\n",
    "    return factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation_coefficient(array1, array2):\n",
    "    if len(array1) != len(array2):\n",
    "        raise ValueError(\"Arrays must be of the same length\")\n",
    "    \n",
    "    mean1 = np.mean(array1)\n",
    "    mean2 = np.mean(array2)\n",
    "    \n",
    "    centered1 = array1 - mean1\n",
    "    centered2 = array2 - mean2\n",
    "    \n",
    "    covariance = np.sum(centered1 * centered2) / len(array1)\n",
    "    std_dev1 = np.sqrt(np.sum(centered1**2) / len(array1))\n",
    "    std_dev2 = np.sqrt(np.sum(centered2**2) / len(array2))\n",
    "    \n",
    "    if std_dev1 * std_dev2 == 0:\n",
    "        return 0\n",
    "    \n",
    "    pearson_coefficient = covariance / (std_dev1 * std_dev2)\n",
    "    \n",
    "    return pearson_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(user_vector, similar_users_count, buckets, min_hash, sparse_matrix):\n",
    "    nm = create_neighborhood_matrix(user_vector, buckets, min_hash, sparse_matrix)\n",
    "    min_shape = np.min(nm.shape)\n",
    "    if min_shape == 0:\n",
    "        return []\n",
    "    k = int(0.5 * min_shape)\n",
    "    if k < 1:\n",
    "        k = 1\n",
    "    if k == min_shape:\n",
    "       return [] \n",
    "    neighborhood_matrix = matrix_factorization(nm, k)\n",
    "    \n",
    "    similarities = np.array([np.abs(pearson_correlation_coefficient(neighborhood_matrix[i], user_vector))\n",
    "                             for i in range(neighborhood_matrix.shape[0])])\n",
    "    \n",
    "    similar_users = similarities.argsort()[::-1]\n",
    "    user_mean = user_vector[user_vector != 0].mean()\n",
    "    \n",
    "    predicts = []\n",
    "    for item_idx in range(neighborhood_matrix.shape[1]):\n",
    "        if user_vector[item_idx] == 0:\n",
    "            predicts.append(0)\n",
    "            continue\n",
    "        weighted_ratings_sum = 0\n",
    "        weights_sum = 0\n",
    "        users_added = 0\n",
    "        for user_index in similar_users:\n",
    "            if users_added == similar_users_count:\n",
    "                break\n",
    "            \n",
    "            user_rating = neighborhood_matrix[user_index, item_idx]\n",
    "            if user_rating == 0:\n",
    "                continue\n",
    "            users_added += 1\n",
    "            similarity_score = similarities[user_index]\n",
    "            weighted_ratings_sum += user_rating * similarity_score\n",
    "            weights_sum += np.abs(similarity_score)\n",
    "        \n",
    "        predicted_rating = weighted_ratings_sum / weights_sum if weights_sum > 0 else 0\n",
    "        predicts.append(predicted_rating + user_mean)\n",
    "    \n",
    "    return np.array(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_sparse(rating_df, df, key= 'normalized_rating'):\n",
    "    unique_user_ids = pd.Series(rating_df['userId'].unique()).sort_values()\n",
    "    user_mapping = pd.Series(index=unique_user_ids, data=range(len(unique_user_ids)))\n",
    "\n",
    "    unique_movie_ids = pd.Series(rating_df['movieId'].unique()).sort_values()\n",
    "    movie_mapping = pd.Series(index=unique_movie_ids, data=range(len(unique_movie_ids)))\n",
    "    \n",
    "    user_indices = df['userId'].map(user_mapping)\n",
    "    movie_indices = df['movieId'].map(movie_mapping)\n",
    "    values = df[key]\n",
    "\n",
    "    result = coo_matrix((values, (user_indices, movie_indices)), \n",
    "                    shape=(len(user_mapping), len(movie_mapping)))\n",
    "    print(result.shape)\n",
    "    print(result.count_nonzero())\n",
    "    return result.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138493, 26744)\n",
      "17935929\n",
      "(138493, 26744)\n",
      "1985928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bucketing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 138493/138493 [00:39<00:00, 3529.05it/s]\n",
      "Evaluating:  19%|â–ˆâ–‰        | 2683/13849 [04:34<19:02,  9.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_vector\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m predicted \u001b[38;5;241m=\u001b[39m predict_ratings(user_vector, \u001b[38;5;241m20\u001b[39m, buckets, min_hash, train_sparse_matrix)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predicted) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mpredict_ratings\u001b[0;34m(user_vector, similar_users_count, buckets, min_hash, sparse_matrix)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m min_shape:\n\u001b[1;32m     10\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m [] \n\u001b[0;32m---> 11\u001b[0m neighborhood_matrix \u001b[38;5;241m=\u001b[39m matrix_factorization(nm, k)\n\u001b[1;32m     13\u001b[0m similarities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mabs(pearson_correlation_coefficient(neighborhood_matrix[i], user_vector))\n\u001b[1;32m     14\u001b[0m                          \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(neighborhood_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])])\n\u001b[1;32m     16\u001b[0m similar_users \u001b[38;5;241m=\u001b[39m similarities\u001b[38;5;241m.\u001b[39margsort()[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mmatrix_factorization\u001b[0;34m(sparse_ratings, k)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatrix_factorization\u001b[39m(sparse_ratings: csr_matrix, k):\n\u001b[0;32m----> 9\u001b[0m     U, S, V \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msvds(sparse_ratings, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m     10\u001b[0m     item_factors \u001b[38;5;241m=\u001b[39m U\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mdiag(S))\n\u001b[1;32m     11\u001b[0m     user_factors \u001b[38;5;241m=\u001b[39m V\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/.conda/envs/ml/lib/python3.11/site-packages/scipy/sparse/linalg/_eigen/_svds.py:515\u001b[0m, in \u001b[0;36msvds\u001b[0;34m(A, k, ncv, tol, which, v0, maxiter, return_singular_vectors, solver, random_state, options)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m v0 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    514\u001b[0m     v0 \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mstandard_normal(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mmin\u001b[39m(A\u001b[38;5;241m.\u001b[39mshape),))\n\u001b[0;32m--> 515\u001b[0m _, eigvec \u001b[38;5;241m=\u001b[39m eigsh(XH_X, k\u001b[38;5;241m=\u001b[39mk, tol\u001b[38;5;241m=\u001b[39mtol \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, maxiter\u001b[38;5;241m=\u001b[39mmaxiter,\n\u001b[1;32m    516\u001b[0m                   ncv\u001b[38;5;241m=\u001b[39mncv, which\u001b[38;5;241m=\u001b[39mwhich, v0\u001b[38;5;241m=\u001b[39mv0)\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# arpack do not guarantee exactly orthonormal eigenvectors\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# for clustered eigenvalues, especially in complex arithmetic\u001b[39;00m\n\u001b[1;32m    519\u001b[0m eigvec, _ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mqr(eigvec)\n",
      "File \u001b[0;32m~/.conda/envs/ml/lib/python3.11/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1700\u001b[0m, in \u001b[0;36meigsh\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _ARPACK_LOCK:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m params\u001b[38;5;241m.\u001b[39mconverged:\n\u001b[0;32m-> 1700\u001b[0m         params\u001b[38;5;241m.\u001b[39miterate()\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m params\u001b[38;5;241m.\u001b[39mextract(return_eigenvectors)\n",
      "File \u001b[0;32m~/.conda/envs/ml/lib/python3.11/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:537\u001b[0m, in \u001b[0;36m_SymmetricArpackParams.iterate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mido, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miparam, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipntr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 537\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arpack_solver(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mido, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbmat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhich, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk,\n\u001b[1;32m    538\u001b[0m                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miparam,\n\u001b[1;32m    539\u001b[0m                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipntr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkl, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo)\n\u001b[1;32m    541\u001b[0m     xslice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipntr[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipntr[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m    542\u001b[0m     yslice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipntr[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipntr[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "unique_users = rating_df['userId'].unique()\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "total_loss = 0\n",
    "for train_index, val_index in kf.split(unique_users):\n",
    "    time.sleep(10)\n",
    "    train_users = unique_users[train_index]\n",
    "    val_users = unique_users[val_index]\n",
    "    \n",
    "    # Filter data based on split users\n",
    "    train_data = rating_df[rating_df['userId'].isin(train_users)]\n",
    "    val_data = rating_df[rating_df['userId'].isin(val_users)]\n",
    "\n",
    "    # Normalize train data and create sparse matrices\n",
    "    train_data, mean_ratings_train = normalize_data(train_data)\n",
    "    train_sparse_matrix = df_to_sparse(rating_df, train_data)\n",
    "\n",
    "    # Normalize val data (if needed) and create sparse matrix\n",
    "    # val_data, mean_ratings_val = normalize_data(val_data)\n",
    "    val_sparse_matrix = df_to_sparse(rating_df, val_data, 'rating')\n",
    "\n",
    "    # Create hash vectors and buckets\n",
    "    min_hash = create_hash_vectors(train_sparse_matrix.shape[1], 14)\n",
    "    buckets = {}\n",
    "    for i in tqdm(range(train_sparse_matrix.shape[0]), desc='Bucketing'):\n",
    "        user = train_sparse_matrix[i].toarray()[0]\n",
    "        hash_vector = create_hash(user, min_hash)\n",
    "        bucket_number = calculate_bucket_number(hash_vector)\n",
    "        if bucket_number not in buckets:\n",
    "            buckets[bucket_number] = []\n",
    "        buckets[bucket_number].append(i)\n",
    "\n",
    "    desire_indices = []\n",
    "    for i in range(val_sparse_matrix.shape[0]):\n",
    "        ith = val_sparse_matrix[i]\n",
    "        if ith.count_nonzero() > 0:\n",
    "            desire_indices.append(i)\n",
    "\n",
    "    loss = 0\n",
    "    for i in tqdm(desire_indices, desc='Evaluating'):\n",
    "        user_vector = val_sparse_matrix[i].toarray()[0]\n",
    "        if user_vector.sum() == 0:\n",
    "            continue\n",
    "        predicted = predict_ratings(user_vector, 20, buckets, min_hash, train_sparse_matrix)\n",
    "        if len(predicted) == 0:\n",
    "            continue\n",
    "\n",
    "        mask = user_vector != 0\n",
    "        actual = user_vector[mask]\n",
    "        pred = predicted[mask]\n",
    "        user_loss = np.mean((actual - pred) ** 2)\n",
    "        loss += user_loss\n",
    "\n",
    "    loss /= len(desire_indices)\n",
    "    total_loss += loss\n",
    "\n",
    "    print(f'Validation Loss: {loss}')\n",
    "\n",
    "average_loss = total_loss / n_splits\n",
    "print(f'Average Cross-Validation Loss: {average_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
