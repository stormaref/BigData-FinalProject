{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import implicit\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = pd.read_csv('Dataset/ratings.csv').astype(int)\n",
    "rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_matrix(data):\n",
    "    pivot_table = data.pivot(index='userId', columns='movieId', values='normalized_rating').fillna(0)\n",
    "    return csr_matrix(pivot_table.values)\n",
    "\n",
    "def normalize_data(data):\n",
    "    mean_ratings = data.groupby('userId')['rating'].mean().reset_index(name='mean_rating')\n",
    "    data = data.merge(mean_ratings, on='userId')\n",
    "    data['normalized_rating'] = data['rating'] - data['mean_rating']\n",
    "    data.drop(columns=['rating', 'mean_rating'], inplace=True, errors='ignore')\n",
    "    return data, mean_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_vectors(length: int, count: int) -> list:\n",
    "    return [np.random.uniform(-1, 1, size=length) for _ in range(count)]\n",
    "        \n",
    "def create_hash(vector: list, min_hash):\n",
    "    result = []\n",
    "    for hash in min_hash:\n",
    "        a = np.dot(vector, hash) > 0\n",
    "        result.append(1 if a else 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bucket_number(hash_vector):\n",
    "    s = ''.join(str(bit) for bit in hash_vector)\n",
    "    bucket_number = int(s, 2)\n",
    "    return bucket_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neighborhood_matrix(new_user_vector, buckets, min_hash, sparse_matrix) -> csr_matrix:\n",
    "    hash_vector = create_hash(new_user_vector, min_hash)\n",
    "    bucket_number = calculate_bucket_number(hash_vector)\n",
    "\n",
    "    if bucket_number in buckets:\n",
    "        neighbor_users = buckets[bucket_number]\n",
    "    else:\n",
    "        return csr_matrix((0, sparse_matrix.shape[1]))\n",
    "    \n",
    "    rows = [sparse_matrix[n].toarray()[0] for n in neighbor_users]\n",
    "    return csr_matrix(np.vstack(rows)) if rows else csr_matrix((0, sparse_matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def matrix_factorization(sparse_ratings: csr_matrix):\n",
    "#     model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=64, use_native=True, num_threads=0)\n",
    "#     model.fit(sparse_ratings.T, show_progress=False)\n",
    "#     user_factors = model.user_factors\n",
    "#     item_factors = model.item_factors\n",
    "#     return item_factors @ user_factors.T\n",
    "\n",
    "def matrix_factorization(sparse_ratings: csr_matrix, k):\n",
    "    U, S, V = scipy.sparse.linalg.svds(sparse_ratings, k=k)\n",
    "    item_factors = U.dot(np.diag(S))\n",
    "    user_factors = V.T\n",
    "    factorization = item_factors.dot(user_factors.T)\n",
    "    factorization = item_factors @ user_factors.T\n",
    "    return factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation_coefficient(array1, array2):\n",
    "    if len(array1) != len(array2):\n",
    "        raise ValueError(\"Arrays must be of the same length\")\n",
    "    \n",
    "    mean1 = np.mean(array1)\n",
    "    mean2 = np.mean(array2)\n",
    "    \n",
    "    centered1 = array1 - mean1\n",
    "    centered2 = array2 - mean2\n",
    "    \n",
    "    covariance = np.sum(centered1 * centered2) / len(array1)\n",
    "    std_dev1 = np.sqrt(np.sum(centered1**2) / len(array1))\n",
    "    std_dev2 = np.sqrt(np.sum(centered2**2) / len(array2))\n",
    "    \n",
    "    pearson_coefficient = covariance / (std_dev1 * std_dev2)\n",
    "    \n",
    "    return pearson_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(user_vector, similar_users_count, buckets, min_hash, sparse_matrix):\n",
    "    nm = create_neighborhood_matrix(user_vector, buckets, min_hash, sparse_matrix)\n",
    "    k = int(0.5 * np.min(nm.shape))\n",
    "    neighborhood_matrix = matrix_factorization(nm, k)\n",
    "    \n",
    "    similarities = np.array([np.abs(pearson_correlation_coefficient(neighborhood_matrix[i], user_vector))\n",
    "                             for i in range(neighborhood_matrix.shape[0])])\n",
    "    \n",
    "    similar_users = similarities.argsort()[::-1]\n",
    "    user_mean = user_vector[user_vector != 0].mean()\n",
    "    \n",
    "    predicts = []\n",
    "    for item_idx in range(neighborhood_matrix.shape[1]):\n",
    "        if user_vector[item_idx] == 0:\n",
    "            predicts.append(0)\n",
    "            continue\n",
    "        weighted_ratings_sum = 0\n",
    "        weights_sum = 0\n",
    "        users_added = 0\n",
    "        for user_index in similar_users:\n",
    "            if users_added == similar_users_count:\n",
    "                break\n",
    "            \n",
    "            user_rating = neighborhood_matrix[user_index, item_idx]\n",
    "            if user_rating == 0:\n",
    "                continue\n",
    "            users_added += 1\n",
    "            similarity_score = similarities[user_index]\n",
    "            weighted_ratings_sum += user_rating * similarity_score\n",
    "            weights_sum += np.abs(similarity_score)\n",
    "        \n",
    "        predicted_rating = weighted_ratings_sum / weights_sum if weights_sum > 0 else 0\n",
    "        predicts.append(predicted_rating + user_mean)\n",
    "    \n",
    "    return np.array(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_sparse(rating_df, df, key= 'normalized_rating'):\n",
    "    unique_user_ids = pd.Series(rating_df['userId'].unique()).sort_values()\n",
    "    user_mapping = pd.Series(index=unique_user_ids, data=range(len(unique_user_ids)))\n",
    "\n",
    "    unique_movie_ids = pd.Series(rating_df['movieId'].unique()).sort_values()\n",
    "    movie_mapping = pd.Series(index=unique_movie_ids, data=range(len(unique_movie_ids)))\n",
    "    \n",
    "    user_indices = df['userId'].map(user_mapping)\n",
    "    movie_indices = df['movieId'].map(movie_mapping)\n",
    "    values = df[key]\n",
    "\n",
    "    result = coo_matrix((values, (user_indices, movie_indices)), \n",
    "                    shape=(len(user_mapping), len(movie_mapping)))\n",
    "    print(result.shape)\n",
    "    print(result.count_nonzero())\n",
    "    return result.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_splits = 5\n",
    "# kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# fold = 1\n",
    "# total_loss = 0\n",
    "# for train_index, val_index in kf.split(rating_df):\n",
    "#     train_data = rating_df.iloc[train_index]\n",
    "#     val_data = rating_df.iloc[val_index]\n",
    "    \n",
    "#     train_data, mean_ratings_train = normalize_data(train_data)\n",
    "    \n",
    "#     # Normalize validation data using training set's mean ratings\n",
    "#     # val_data = val_data.merge(mean_ratings_train, on='userId', how='left')\n",
    "#     # val_data['normalized_rating'] = val_data['rating'] - val_data['mean_rating']\n",
    "#     # val_data['normalized_rating'].fillna(val_data['rating'] - val_data['rating'].mean(), inplace=True)\n",
    "#     # val_data.drop(columns=['rating', 'mean_rating'], inplace=True, errors='ignore')\n",
    "    \n",
    "#     train_sparse_matrix = df_to_sparse(rating_df, train_data)\n",
    "#     val_sparse_matrix = df_to_sparse(rating_df, val_data, 'rating')\n",
    "    \n",
    "#     # Create hash vectors and buckets\n",
    "#     min_hash = create_hash_vectors(train_sparse_matrix.shape[1], 12)\n",
    "#     buckets = {}\n",
    "#     for i in tqdm(range(train_sparse_matrix.shape[0]), desc=f'Fold {fold} - Bucketing'):\n",
    "#         user = train_sparse_matrix[i].toarray()[0]\n",
    "#         hash_vector = create_hash(user, min_hash)\n",
    "#         bucket_number = calculate_bucket_number(hash_vector)\n",
    "#         if bucket_number not in buckets:\n",
    "#             buckets[bucket_number] = []\n",
    "#         buckets[bucket_number].append(i)\n",
    "        \n",
    "        \n",
    "#     desire_indices = []\n",
    "#     for i in range(val_sparse_matrix.shape[0]):\n",
    "#         ith = val_sparse_matrix[i]\n",
    "#         if ith.count_nonzero() > 0:\n",
    "#             desire_indices.append(i)\n",
    "    \n",
    "#     # Evaluate on validation set\n",
    "#     loss = 0\n",
    "#     for i in tqdm(desire_indices, desc=f'Fold {fold} - Evaluating'):\n",
    "#         user_vector: np.ndarray = val_sparse_matrix[i].toarray()[0]\n",
    "#         if user_vector.sum() == 0:\n",
    "#             continue\n",
    "#         predicted = predict_ratings(user_vector, 20, buckets, min_hash, train_sparse_matrix)\n",
    "        \n",
    "#         mask = user_vector != 0\n",
    "#         actual = user_vector[mask]\n",
    "#         pred = predicted[mask]\n",
    "#         user_loss = np.mean((actual - pred) ** 2)\n",
    "#         loss += user_loss\n",
    "    \n",
    "#     loss /= len(val_data['userId'].unique())\n",
    "#     total_loss += loss\n",
    "#     print(f'Fold {fold} Loss: {loss}')\n",
    "#     fold += 1\n",
    "\n",
    "# total_loss /= n_splits\n",
    "# print(f'Average Cross-Validation Loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users = rating_df['userId'].unique()\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "total_loss = 0\n",
    "for train_index, val_index in kf.split(unique_users):\n",
    "    train_users = unique_users[train_index]\n",
    "    val_users = unique_users[val_index]\n",
    "    \n",
    "    # Filter data based on split users\n",
    "    train_data = rating_df[rating_df['userId'].isin(train_users)]\n",
    "    val_data = rating_df[rating_df['userId'].isin(val_users)]\n",
    "\n",
    "    # Normalize train data and create sparse matrices\n",
    "    train_data, mean_ratings_train = normalize_data(train_data)\n",
    "    train_sparse_matrix = df_to_sparse(rating_df, train_data)\n",
    "\n",
    "    # Normalize val data (if needed) and create sparse matrix\n",
    "    # val_data, mean_ratings_val = normalize_data(val_data)\n",
    "    val_sparse_matrix = df_to_sparse(rating_df, val_data, 'rating')\n",
    "\n",
    "    # Create hash vectors and buckets\n",
    "    min_hash = create_hash_vectors(train_sparse_matrix.shape[1], 12)\n",
    "    buckets = {}\n",
    "    for i in tqdm(range(train_sparse_matrix.shape[0]), desc='Bucketing'):\n",
    "        user = train_sparse_matrix[i].toarray()[0]\n",
    "        hash_vector = create_hash(user, min_hash)\n",
    "        bucket_number = calculate_bucket_number(hash_vector)\n",
    "        if bucket_number not in buckets:\n",
    "            buckets[bucket_number] = []\n",
    "        buckets[bucket_number].append(i)\n",
    "\n",
    "    desire_indices = []\n",
    "    for i in range(val_sparse_matrix.shape[0]):\n",
    "        ith = val_sparse_matrix[i]\n",
    "        if ith.count_nonzero() > 0:\n",
    "            desire_indices.append(i)\n",
    "\n",
    "    loss = 0\n",
    "    for i in tqdm(desire_indices, desc='Evaluating'):\n",
    "        user_vector = val_sparse_matrix[i].toarray()[0]\n",
    "        if user_vector.sum() == 0:\n",
    "            continue\n",
    "        predicted = predict_ratings(user_vector, 20, buckets, min_hash, train_sparse_matrix)\n",
    "\n",
    "        mask = user_vector != 0\n",
    "        actual = user_vector[mask]\n",
    "        pred = predicted[mask]\n",
    "        user_loss = np.mean((actual - pred) ** 2)\n",
    "        loss += user_loss\n",
    "\n",
    "    loss /= len(val_data['userId'].unique())\n",
    "    total_loss += loss\n",
    "\n",
    "    print(f'Validation Loss: {loss}')\n",
    "\n",
    "average_loss = total_loss / n_splits\n",
    "print(f'Average Cross-Validation Loss: {average_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
